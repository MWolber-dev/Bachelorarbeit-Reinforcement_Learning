{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c241b78b-f9c5-4eea-b951-12a12099bf11",
   "metadata": {},
   "source": [
    "# Notwendige Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38925c45-abd0-44bc-9393-fb1cd35494f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install gymnasium\n",
    "!pip install stable-baselines3\n",
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc8db5-12ed-462a-afef-4e5652cf044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecTransposeImage\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import HParam\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import gc\n",
    "\n",
    "import os\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3716d-7451-472e-8040-f69a4018ffc5",
   "metadata": {},
   "source": [
    "# Einige variable Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b811f-849f-4dfd-8e3b-4eea0026c5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_steps = 50000000\n",
    "experiment = \"training_6/DQN\"\n",
    "kombinationen_path = \"./3_kombinationen_DQN.csv\"\n",
    "ergebnisse_path = \"./training_bayesian.csv\"\n",
    "loop_start = 0\n",
    "loop_end = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca7871-0e40-49b2-96c3-f3eff1b5d7b9",
   "metadata": {},
   "source": [
    "# Konstanten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd1b50-e422-4228-87f4-c1862674b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = \"ALE/Pacman-v5\"\n",
    "frameskip = 3\n",
    "image_size = 84\n",
    "frame_stack = 4\n",
    "input_shape = (4, 84, 84)\n",
    "target_update_interval = 10000\n",
    "eval_episodes = 30\n",
    "buffer_size = 250000\n",
    "custom_eval_freq = 100000\n",
    "LOG_DIR = f\"./experiments/{experiment}/logs/\"\n",
    "BEST_MODEL_LOG_DIR = f\"./experiments/{experiment}/logs/best_model/\"\n",
    "EVAL_ENV_LOG_DIR = f\"./experiments/{experiment}/logs/eval_log/\"\n",
    "BEST_MODEL_DIR = f\"./experiments/{experiment}/train/\"\n",
    "MODEL_WEIGHTS_FILE = \"best_model_weights.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea075e2-7fb7-44d8-943a-dac510d8f3f8",
   "metadata": {},
   "source": [
    "# Eigene CNN-Klasse und Callback-Klassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9cf61-af37-4295-a724-85c65ed45feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(self.calculate_conv_output_dim(input_shape), 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values\n",
    "\n",
    "    def calculate_conv_output_dim(self, input_shape):\n",
    "        dummy_input = torch.zeros(1, *input_shape)\n",
    "        dummy_output = self.conv3(self.conv2(self.conv1(dummy_input)))\n",
    "        return int(torch.prod(torch.tensor(dummy_output.size())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d53e7a-4864-43cd-9757-a90f34b8f4d8",
   "metadata": {},
   "source": [
    "## Loggin des Rewards zu Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4566dfb4-8d45-4130-ad4d-8f72ab76cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardTensorboardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(RewardTensorboardCallback, self).__init__(verbose)\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        reward = self.locals[\"rewards\"]\n",
    "        self.total_reward += np.sum(reward)\n",
    "\n",
    "        if self.locals[\"dones\"][0]:\n",
    "            self.logger.record(\"total_reward_steps\", self.total_reward)\n",
    "            \n",
    "            self.total_reward = 0\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3fafe-62c9-45fc-b1d5-5cdf96d44bba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logging der Hyperparameter zu Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e758f-ab3d-4790-937e-5a60dc2a6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParamCallback(BaseCallback):\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        hparam_dict = {\n",
    "            \"algorithm\": self.model.__class__.__name__,\n",
    "            \"learning rate\": self.model.learning_rate,\n",
    "            \"gamma\": self.model.gamma,\n",
    "            \"batch size\": self.model.batch_size,\n",
    "            \"exploration fraction\": self.model.exploration_fraction,\n",
    "            \"start exploration\": self.model.exploration_initial_eps,\n",
    "            \"final exploration\": self.model.exploration_final_eps            \n",
    "        }\n",
    "\n",
    "        metric_dict = {\n",
    "            \"rollout/ep_len_mean\": 0,\n",
    "            \"train/value_loss\": 0.0,\n",
    "        }\n",
    "        self.logger.record(\n",
    "            \"hparams\",\n",
    "            HParam(hparam_dict, metric_dict),\n",
    "            exclude=(\"stdout\", \"log\", \"json\", \"csv\"),\n",
    "        )\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f904301-549c-48c1-a547-c02e2dc42a94",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kopierter Eval-Callback der Bibliothek mit leichten Anpassungen bei der Bewertung und Speicherung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52152381-1020-4fdd-bcdd-5a4937e524db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEvalCallback(EvalCallback):\n",
    "    def _on_step(self) -> bool:\n",
    "        continue_training = True\n",
    "\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Sync training and eval env if there is VecNormalize\n",
    "            if self.model.get_vec_normalize_env() is not None:\n",
    "                try:\n",
    "                    sync_envs_normalization(self.training_env, self.eval_env)\n",
    "                except AttributeError as e:\n",
    "                    raise AssertionError(\n",
    "                        \"Training and eval env are not wrapped the same way, \"\n",
    "                        \"see https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html#evalcallback \"\n",
    "                        \"and warning above.\"\n",
    "                    ) from e\n",
    "\n",
    "            # Reset success rate buffer\n",
    "            self._is_success_buffer = []\n",
    "\n",
    "            episode_rewards, episode_lengths = evaluate_policy(\n",
    "                self.model,\n",
    "                self.eval_env,\n",
    "                n_eval_episodes=self.n_eval_episodes,\n",
    "                render=self.render,\n",
    "                deterministic=self.deterministic,\n",
    "                return_episode_rewards=True,\n",
    "                warn=self.warn,\n",
    "                callback=self._log_success_callback,\n",
    "            )\n",
    "\n",
    "            if self.log_path is not None:\n",
    "                self.evaluations_timesteps.append(self.num_timesteps)\n",
    "                self.evaluations_results.append(episode_rewards)\n",
    "                self.evaluations_length.append(episode_lengths)\n",
    "\n",
    "                kwargs = {}\n",
    "                # Save success log if present\n",
    "                if len(self._is_success_buffer) > 0:\n",
    "                    self.evaluations_successes.append(self._is_success_buffer)\n",
    "                    kwargs = dict(successes=self.evaluations_successes)\n",
    "\n",
    "                np.savez(\n",
    "                    self.log_path,\n",
    "                    timesteps=self.evaluations_timesteps,\n",
    "                    results=self.evaluations_results,\n",
    "                    ep_lengths=self.evaluations_length,\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "            mean_reward, std_reward = np.mean(episode_rewards), np.std(episode_rewards)\n",
    "            mean_ep_length, std_ep_length = np.mean(episode_lengths), np.std(episode_lengths)\n",
    "            self.last_mean_reward = mean_reward\n",
    "\n",
    "            if self.verbose >= 1:\n",
    "                print(f\"Eval num_timesteps={self.num_timesteps}, \" f\"episode_reward={mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "                print(f\"Episode length: {mean_ep_length:.2f} +/- {std_ep_length:.2f}\")\n",
    "            # Add to current Logger\n",
    "            self.logger.record(\"eval/mean_reward\", float(mean_reward))\n",
    "            self.logger.record(\"eval/mean_ep_length\", mean_ep_length)\n",
    "\n",
    "            if len(self._is_success_buffer) > 0:\n",
    "                success_rate = np.mean(self._is_success_buffer)\n",
    "                if self.verbose >= 1:\n",
    "                    print(f\"Success rate: {100 * success_rate:.2f}%\")\n",
    "                self.logger.record(\"eval/success_rate\", success_rate)\n",
    "\n",
    "            # Dump log so the evaluation results are printed with the correct timestep\n",
    "            self.logger.record(\"time/total_timesteps\", self.num_timesteps, exclude=\"tensorboard\")\n",
    "            self.logger.dump(self.num_timesteps)\n",
    "\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                if self.verbose >= 1:\n",
    "                    print(\"New best mean reward!\")\n",
    "                if self.best_model_save_path is not None:\n",
    "                    self.model.save(os.path.join(self.best_model_save_path, \"best_model\"))\n",
    "                    torch.save(model.policy.state_dict(), os.path.join(self.best_model_save_path, \"best_model_weights.pth\"))\n",
    "                self.best_mean_reward = mean_reward\n",
    "                # Trigger callback on new best model, if needed\n",
    "                if self.callback_on_new_best is not None:\n",
    "                    continue_training = self.callback_on_new_best.on_step()\n",
    "\n",
    "            # Trigger callback after every evaluation, if needed\n",
    "            if self.callback is not None:\n",
    "                continue_training = continue_training and self._on_event()\n",
    "\n",
    "        return continue_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9c166-66d5-41a6-bfdc-540cf0e11fce",
   "metadata": {},
   "source": [
    "# Kombinationen einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b5116-5bc2-4396-8dfa-2879a8f536d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kombinationen = []\n",
    "\n",
    "with open(kombinationen_path, 'r') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        row = [int(val) if val.isdigit() else float(val.replace(',', '.')) for val in row]\n",
    "        kombinationen.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f93e75-5620-4578-8da2-06e9188d0260",
   "metadata": {},
   "source": [
    "# Erstellung der Trainungs- und Testumgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16881c4d-95c7-4e94-bd4f-6a9de7ebd632",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(game, frameskip=frameskip)\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "env = ResizeObservation(env, image_size)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, frame_stack, channels_order=\"last\")\n",
    "\n",
    "eval_env = gym.make(game, frameskip=frameskip)\n",
    "eval_env = Monitor(eval_env, EVAL_ENV_LOG_DIR)\n",
    "eval_env = GrayScaleObservation(eval_env, keep_dim=True)\n",
    "eval_env = ResizeObservation(eval_env, image_size)\n",
    "eval_env = DummyVecEnv([lambda: eval_env])\n",
    "eval_env = VecFrameStack(eval_env, frame_stack, channels_order=\"last\")\n",
    "eval_env = VecTransposeImage(eval_env)\n",
    "\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472504f-b59c-46d9-9b2c-a328c7ef6188",
   "metadata": {},
   "source": [
    "# Training + Bewertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f9999-f04d-412f-96de-9306ce9083d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(loop_start, loop_end):\n",
    "    hparam_callback = HParamCallback()\n",
    "    reward_callback = RewardTensorboardCallback()\n",
    "    eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f\"{BEST_MODEL_DIR}{str(i)}/\",\n",
    "                                      log_path=f\"{BEST_MODEL_LOG_DIR}{str(i)}/\", eval_freq=custom_eval_freq,\n",
    "                                      deterministic=True, render=False, n_eval_episodes=eval_episodes)\n",
    "    callback_list = CallbackList([eval_callback, reward_callback, hparam_callback])\n",
    "    \n",
    "    learning_rate = kombinationen[i][2]\n",
    "    gamma =  kombinationen[i][3]\n",
    "    batch_size = kombinationen[i][4]\n",
    "    exploration_initial_eps = kombinationen[i][5]\n",
    "    exploration_final_eps = kombinationen[i][6]\n",
    "    exploration_fraction = kombinationen[i][7]\n",
    "    \n",
    "    # Modell erstellen\n",
    "    model = DQN('CnnPolicy', env, gamma=gamma, learning_rate=learning_rate, verbose=2,\n",
    "            target_update_interval=target_update_interval, buffer_size=buffer_size, \n",
    "            learning_starts=target_update_interval, batch_size=batch_size, tensorboard_log=f\"{LOG_DIR}{str(i)}/\",\n",
    "            exploration_fraction=exploration_fraction, exploration_final_eps=exploration_final_eps,\n",
    "            exploration_initial_eps=exploration_initial_eps)\n",
    "    model.policy.features_extractor = CustomCNN(input_shape, num_actions)\n",
    "    # Modell trainieren\n",
    "    model.learn(total_timesteps=training_steps, callback=callback_list)\n",
    "    \n",
    "    model.replay_buffer.reset()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bestes Modell laden\n",
    "    best_model = DQN('CnnPolicy', env, verbose=1, buffer_size=0)\n",
    "    best_model.policy.features_extractor = CustomCNN(input_shape, num_actions)\n",
    "    state_dict = torch.load(f\"{BEST_MODEL_DIR}{str(i)}/{MODEL_WEIGHTS_FILE}\")\n",
    "    best_model.policy.load_state_dict(state_dict)\n",
    "    \n",
    "    # Modell evaluieren\n",
    "    observation = env.reset()\n",
    "    episodes = 0\n",
    "    done = False\n",
    "    all_rewards = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(f\"Start evaluation Nr. {i}:\")\n",
    "\n",
    "    while episodes < eval_episodes:\n",
    "        if done:\n",
    "            observation = env.reset()\n",
    "            all_rewards.append(total_reward)\n",
    "            episodes += 1\n",
    "            print(\"Episode\", episodes, \"Reward:\", total_reward)\n",
    "            total_reward = 0\n",
    "\n",
    "        action, _ = best_model.predict(observation, deterministic=True)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    env.reset()\n",
    "    \n",
    "    del best_model    \n",
    "    gc.collect()\n",
    "\n",
    "    mean_reward = np.mean(all_rewards)\n",
    "    std_reward = np.std(all_rewards)\n",
    "\n",
    "    print(\"Durchschnittliche Belohnung:\", mean_reward)\n",
    "    print(\"Standardabweichung der Belohnungen:\", std_reward)\n",
    "    \n",
    "    # Ergebnisse in CSV-Datei schreiben\n",
    "    neue_zeile = [i, mean_reward, std_reward]\n",
    "\n",
    "    with open(ergebnisse_path, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(neue_zeile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
