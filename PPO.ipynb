{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d88f64-2164-4960-8aaf-ab53b58de174",
   "metadata": {},
   "source": [
    "# Notwendige Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32650df2-b870-4e84-9ddf-20607040a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install gymnasium\n",
    "!pip install stable-baselines3\n",
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc8db5-12ed-462a-afef-4e5652cf044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecTransposeImage\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import HParam\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import gc\n",
    "\n",
    "import os\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d3716d-7451-472e-8040-f69a4018ffc5",
   "metadata": {},
   "source": [
    "# Variable Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b811f-849f-4dfd-8e3b-4eea0026c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = 5000000\n",
    "custom_eval_freq = 100000\n",
    "eval_episodes = 30\n",
    "experiment = \"training_3/PPO\"\n",
    "kombinationen_path = \"./3_kombinationen_PPO.csv\"\n",
    "ergebnisse_path = \"./3_ergebnisse_PPO.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ddb602-4a6a-4533-a50a-8d053f7cd32f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_update_interval = 10000\n",
    "loop_start = 0\n",
    "vf_coef = 1\n",
    "clip_range = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca7871-0e40-49b2-96c3-f3eff1b5d7b9",
   "metadata": {},
   "source": [
    "# UnverÃ¤nderte Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd1b50-e422-4228-87f4-c1862674b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = \"ALE/Pacman-v5\"\n",
    "frameskip = 3\n",
    "image_size = 84\n",
    "frame_stack = 4\n",
    "input_shape = (4, 84, 84)\n",
    "LOG_DIR = f\"./experiments/{experiment}/logs/\"\n",
    "BEST_MODEL_LOG_DIR = f\"./experiments/{experiment}/logs/best_model/\"\n",
    "EVAL_ENV_LOG_DIR = f\"./experiments/{experiment}/logs/eval_log/\"\n",
    "BEST_MODEL_DIR = f\"./experiments/{experiment}/train/\"\n",
    "MODEL_WEIGHTS_FILE = \"best_model.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea075e2-7fb7-44d8-943a-dac510d8f3f8",
   "metadata": {},
   "source": [
    "# Eigene CNN-Klasse und Callback-Klassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4566dfb4-8d45-4130-ad4d-8f72ab76cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardTensorboardCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(RewardTensorboardCallback, self).__init__(verbose)\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        reward = self.locals[\"rewards\"]\n",
    "        self.total_reward += np.sum(reward)\n",
    "\n",
    "        if self.locals[\"dones\"][0]:\n",
    "            self.logger.record(\"total_reward_steps\", self.total_reward)\n",
    "            \n",
    "            self.total_reward = 0\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e758f-ab3d-4790-937e-5a60dc2a6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParamCallback(BaseCallback):\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        hparam_dict = {\n",
    "            \"algorithm\": self.model.__class__.__name__,\n",
    "            \"learning rate\": self.model.learning_rate,\n",
    "            \"gamma\": self.model.gamma,\n",
    "            \"batch size\": self.model.batch_size,\n",
    "            \"entropiekoeffizient\": self.model.ent_coef\n",
    "        }\n",
    "\n",
    "        metric_dict = {\n",
    "            \"rollout/ep_len_mean\": 0,\n",
    "            \"train/value_loss\": 0.0,\n",
    "        }\n",
    "        self.logger.record(\n",
    "            \"hparams\",\n",
    "            HParam(hparam_dict, metric_dict),\n",
    "            exclude=(\"stdout\", \"log\", \"json\", \"csv\"),\n",
    "        )\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf82673-f09b-4dbf-9dab-5a9d96c4a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEvalCallback(EvalCallback):\n",
    "    def _on_step(self) -> bool:\n",
    "        continue_training = True\n",
    "\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0 and self.n_calls >= 500000:\n",
    "            # Sync training and eval env if there is VecNormalize\n",
    "            if self.model.get_vec_normalize_env() is not None:\n",
    "                try:\n",
    "                    sync_envs_normalization(self.training_env, self.eval_env)\n",
    "                except AttributeError as e:\n",
    "                    raise AssertionError(\n",
    "                        \"Training and eval env are not wrapped the same way, \"\n",
    "                        \"see https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html#evalcallback \"\n",
    "                        \"and warning above.\"\n",
    "                    ) from e\n",
    "\n",
    "            # Reset success rate buffer\n",
    "            self._is_success_buffer = []\n",
    "\n",
    "            episode_rewards, episode_lengths = evaluate_policy(\n",
    "                self.model,\n",
    "                self.eval_env,\n",
    "                n_eval_episodes=self.n_eval_episodes,\n",
    "                render=self.render,\n",
    "                deterministic=self.deterministic,\n",
    "                return_episode_rewards=True,\n",
    "                warn=self.warn,\n",
    "                callback=self._log_success_callback,\n",
    "            )\n",
    "\n",
    "            if self.log_path is not None:\n",
    "                self.evaluations_timesteps.append(self.num_timesteps)\n",
    "                self.evaluations_results.append(episode_rewards)\n",
    "                self.evaluations_length.append(episode_lengths)\n",
    "\n",
    "                kwargs = {}\n",
    "                # Save success log if present\n",
    "                if len(self._is_success_buffer) > 0:\n",
    "                    self.evaluations_successes.append(self._is_success_buffer)\n",
    "                    kwargs = dict(successes=self.evaluations_successes)\n",
    "\n",
    "                np.savez(\n",
    "                    self.log_path,\n",
    "                    timesteps=self.evaluations_timesteps,\n",
    "                    results=self.evaluations_results,\n",
    "                    ep_lengths=self.evaluations_length,\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "            mean_reward, std_reward = np.mean(episode_rewards), np.std(episode_rewards)\n",
    "            mean_ep_length, std_ep_length = np.mean(episode_lengths), np.std(episode_lengths)\n",
    "            self.last_mean_reward = mean_reward\n",
    "\n",
    "            if self.verbose >= 1:\n",
    "                print(f\"Eval num_timesteps={self.num_timesteps}, \" f\"episode_reward={mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "                print(f\"Episode length: {mean_ep_length:.2f} +/- {std_ep_length:.2f}\")\n",
    "            # Add to current Logger\n",
    "            self.logger.record(\"eval/mean_reward\", float(mean_reward))\n",
    "            self.logger.record(\"eval/mean_ep_length\", mean_ep_length)\n",
    "\n",
    "            if len(self._is_success_buffer) > 0:\n",
    "                success_rate = np.mean(self._is_success_buffer)\n",
    "                if self.verbose >= 1:\n",
    "                    print(f\"Success rate: {100 * success_rate:.2f}%\")\n",
    "                self.logger.record(\"eval/success_rate\", success_rate)\n",
    "\n",
    "            # Dump log so the evaluation results are printed with the correct timestep\n",
    "            self.logger.record(\"time/total_timesteps\", self.num_timesteps, exclude=\"tensorboard\")\n",
    "            self.logger.dump(self.num_timesteps)\n",
    "\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                if self.verbose >= 1:\n",
    "                    print(\"New best mean reward!\")\n",
    "                if self.best_model_save_path is not None:\n",
    "                    self.model.save(os.path.join(self.best_model_save_path, \"best_model\"))\n",
    "                self.best_mean_reward = mean_reward\n",
    "                # Trigger callback on new best model, if needed\n",
    "                if self.callback_on_new_best is not None:\n",
    "                    continue_training = self.callback_on_new_best.on_step()\n",
    "\n",
    "            # Trigger callback after every evaluation, if needed\n",
    "            if self.callback is not None:\n",
    "                continue_training = continue_training and self._on_event()\n",
    "\n",
    "        return continue_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90711b9b-daed-436d-aaa4-080943401b0a",
   "metadata": {},
   "source": [
    "# Kombinationen einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f793624-02f5-48c0-80ca-b86dc97036dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kombinationen = []\n",
    "\n",
    "with open(kombinationen_path, 'r') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        row = [int(val) if val.isdigit() else float(val.replace(',', '.')) for val in row]\n",
    "        kombinationen.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f93e75-5620-4578-8da2-06e9188d0260",
   "metadata": {},
   "source": [
    "# Erstellung der Trainungs- und Testumgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16881c4d-95c7-4e94-bd4f-6a9de7ebd632",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(game, frameskip=frameskip)\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "env = ResizeObservation(env, image_size)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, frame_stack, channels_order=\"last\")\n",
    "\n",
    "eval_env = gym.make(game, frameskip=frameskip)\n",
    "eval_env = Monitor(eval_env, EVAL_ENV_LOG_DIR)\n",
    "eval_env = GrayScaleObservation(eval_env, keep_dim=True)\n",
    "eval_env = ResizeObservation(eval_env, image_size)\n",
    "eval_env = DummyVecEnv([lambda: eval_env])\n",
    "eval_env = VecFrameStack(eval_env, frame_stack, channels_order=\"last\")\n",
    "eval_env = VecTransposeImage(eval_env)\n",
    "\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b13b55-ff15-46bc-a91b-fdec0a0fc3c1",
   "metadata": {},
   "source": [
    "# Erstellung der Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00d4e0-6817-4e49-b3d5-2028a07e87fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam_callback = HParamCallback()\n",
    "reward_callback = RewardTensorboardCallback()\n",
    "eval_callback = CustomEvalCallback(eval_env, best_model_save_path=BEST_MODEL_DIR,\n",
    "                                  log_path=BEST_MODEL_LOG_DIR, eval_freq=custom_eval_freq,\n",
    "                                  deterministic=True, render=False, n_eval_episodes=eval_episodes)\n",
    "callback_list = CallbackList([eval_callback, reward_callback, hparam_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f56d4a-7220-44d3-b19f-a6fe4952eefb",
   "metadata": {},
   "source": [
    "# Training + Bewertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ff848-13ee-4d75-af6b-10c53fa37e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(loop_start, 1):\n",
    "    if i != 1 or i != 3 or i != 6:\n",
    "        hparam_callback = HParamCallback()\n",
    "        reward_callback = RewardTensorboardCallback()\n",
    "        eval_callback = CustomEvalCallback(eval_env, best_model_save_path=f\"{BEST_MODEL_DIR}{str(i)}/\",\n",
    "                                          log_path=f\"{BEST_MODEL_LOG_DIR}{str(i)}/\", eval_freq=custom_eval_freq,\n",
    "                                          deterministic=True, render=False, n_eval_episodes=eval_episodes)\n",
    "        callback_list = CallbackList([eval_callback, reward_callback, hparam_callback])\n",
    "\n",
    "        learning_rate = 0.0001\n",
    "        gamma = 0.95\n",
    "        batch_size = 32\n",
    "        ent_coef = 0.01\n",
    "\n",
    "        # Modell erstellen\n",
    "        model = PPO('CnnPolicy', env, gamma=gamma, learning_rate=learning_rate, verbose=2,            \n",
    "                batch_size=batch_size, tensorboard_log=f\"{LOG_DIR}{str(i)}/\",\n",
    "                ent_coef=ent_coef, vf_coef=vf_coef, clip_range=clip_range)\n",
    "\n",
    "        # Modell trainieren\n",
    "        model.learn(total_timesteps=training_steps, callback=callback_list)\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "\n",
    "        # Bestes Modell laden\n",
    "        best_model = PPO.load(f\"{BEST_MODEL_DIR}{i}/{MODEL_WEIGHTS_FILE}\")\n",
    "\n",
    "        # Modell evaluieren\n",
    "        observation = env.reset()\n",
    "        episodes = 0\n",
    "        done = False\n",
    "        all_rewards = []\n",
    "        total_reward = 0\n",
    "\n",
    "        print(f\"Start evaluation Nr. {i}:\")\n",
    "\n",
    "        while episodes < eval_episodes:\n",
    "            if done:\n",
    "                observation = env.reset()\n",
    "                all_rewards.append(total_reward)\n",
    "                episodes += 1\n",
    "                print(\"Episode\", episodes, \"Reward:\", total_reward)\n",
    "                total_reward = 0\n",
    "\n",
    "            action, _ = best_model.predict(observation, deterministic=True)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        env.reset()\n",
    "\n",
    "        del best_model\n",
    "        gc.collect()\n",
    "\n",
    "        mean_reward = np.mean(all_rewards)\n",
    "        std_reward = np.std(all_rewards)\n",
    "\n",
    "        print(\"Durchschnittliche Belohnung:\", mean_reward)\n",
    "        print(\"Standardabweichung der Belohnungen:\", std_reward)\n",
    "\n",
    "        # Ergebnisse in CSV-Datei schreiben\n",
    "        neue_zeile = [i, mean_reward, std_reward]\n",
    "\n",
    "        with open(ergebnisse_path, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(neue_zeile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
